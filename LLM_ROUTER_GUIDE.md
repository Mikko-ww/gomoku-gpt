# LLM è·¯ç”±å™¨ä½¿ç”¨æŒ‡å—

LLMè·¯ç”±å™¨æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹æ¥å£ï¼Œæ”¯æŒ OpenRouterã€DeepSeek å’Œ xAI ä¸‰ä¸ªå¹³å°ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ”€ **ç»Ÿä¸€æ¥å£**: ä¸€å¥—APIè®¿é—®å¤šä¸ªLLMå¹³å°
- ğŸ¯ **æ™ºèƒ½è·¯ç”±**: è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æä¾›å•†
- âš™ï¸  **çµæ´»é…ç½®**: æ”¯æŒé…ç½®æ–‡ä»¶å’Œç¯å¢ƒå˜é‡
- ğŸ“ **æ ‡å‡†æ ¼å¼**: ç»Ÿä¸€çš„æ¶ˆæ¯å’Œå“åº”æ ¼å¼
- ğŸ”„ **è‡ªåŠ¨åˆ‡æ¢**: æä¾›å•†æ•…éšœæ—¶å¯è‡ªåŠ¨åˆ‡æ¢
- ğŸ›¡ï¸  **é”™è¯¯å¤„ç†**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

## å¿«é€Ÿå¼€å§‹

### 1. é…ç½®APIå¯†é’¥

å¤åˆ¶å¹¶ç¼–è¾‘é…ç½®æ–‡ä»¶ï¼š
```bash
cp config_template.py config.py
```

åœ¨ `config.py` ä¸­å¡«å…¥ä½ çš„APIå¯†é’¥ï¼š
```python
API_KEYS = {
    "openrouter": {
        "api_key": "your_openrouter_api_key",
        "default_model": "openai/gpt-3.5-turbo"
    },
    "deepseek": {
        "api_key": "your_deepseek_api_key", 
        "default_model": "deepseek-chat"
    },
    "xai": {
        "api_key": "your_xai_api_key",
        "default_model": "grok-beta"
    }
}
```

### 2. åŸºæœ¬ä½¿ç”¨

```python
from llm_router import LLMRouter

# åˆå§‹åŒ–è·¯ç”±å™¨
router = LLMRouter()

# ç®€å•èŠå¤©
response = router.simple_chat(
    system="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹",
    user="ä»€ä¹ˆæ˜¯äº”å­æ£‹ï¼Ÿ"
)
print(response)
```

### 3. æŒ‡å®šæä¾›å•†å’Œæ¨¡å‹

```python
# ä½¿ç”¨ç‰¹å®šæä¾›å•†
response = router.simple_chat(
    system="ä½ æ˜¯åŠ©æ‰‹",
    user="è§£é‡Šäººå·¥æ™ºèƒ½",
    provider="deepseek",
    model="deepseek-chat"
)

# ä½¿ç”¨OpenRouterçš„å…è´¹æ¨¡å‹
response = router.simple_chat(
    system="ä½ æ˜¯åŠ©æ‰‹", 
    user="å†™ä¸€é¦–è¯—",
    provider="openrouter",
    model="openai/gpt-oss-20b:free"
)
```

### 4. é«˜çº§ç”¨æ³•

```python
from llm_router import LLMMessage

# ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨æ ¼å¼
messages = [
    LLMMessage(role="system", content="ä½ æ˜¯äº”å­æ£‹ä¸“å®¶"),
    LLMMessage(role="user", content="äº”å­æ£‹å¼€å±€æœ‰ä»€ä¹ˆæŠ€å·§ï¼Ÿ"),
    LLMMessage(role="assistant", content="å¼€å±€å»ºè®®èµ°å¤©å…ƒ..."),
    LLMMessage(role="user", content="é‚£é˜²å®ˆå‘¢ï¼Ÿ")
]

response = router.chat_completion(
    messages=messages,
    temperature=0.8,
    max_tokens=500
)

print(f"å›å¤: {response.content}")
print(f"æ¨¡å‹: {response.model}")
print(f"Tokenä½¿ç”¨: {response.usage}")
```

## é…ç½®é€‰é¡¹

### ç¯å¢ƒå˜é‡é…ç½®
å¦‚æœä¸æƒ³ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œå¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
```bash
export LLM_DEFAULT_PROVIDER="openrouter"
export OPENROUTER_API_KEY="your_key"
export DEEPSEEK_API_KEY="your_key" 
export XAI_API_KEY="your_key"
```

### æ”¯æŒçš„æä¾›å•†å’Œæ¨¡å‹

#### OpenRouter
- `openai/gpt-3.5-turbo` - å¿«é€Ÿï¼Œæ€§ä»·æ¯”é«˜
- `openai/gpt-4` - æœ€å¼ºæ€§èƒ½  
- `anthropic/claude-3-sonnet` - å¹³è¡¡æ€§èƒ½
- `openai/gpt-oss-20b:free` - å…è´¹é€‰é¡¹

#### DeepSeek
- `deepseek-chat` - é€šç”¨èŠå¤©æ¨¡å‹
- `deepseek-coder` - ä»£ç ä¸“ç”¨æ¨¡å‹

#### xAI  
- `grok-beta` - Grok Betaç‰ˆæœ¬

### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| provider | str | None | æŒ‡å®šæä¾›å•† |
| model | str | None | æŒ‡å®šæ¨¡å‹ |
| temperature | float | 0.7 | æ¸©åº¦å‚æ•° |
| max_tokens | int | 1000 | æœ€å¤§tokenæ•° |

## API å‚è€ƒ

### LLMRouter ç±»

#### æ–¹æ³•

- `simple_chat(system, user, provider=None, **kwargs)` - ç®€å•èŠå¤©æ¥å£
- `chat_completion(messages, provider=None, model=None, **kwargs)` - å®Œæ•´èŠå¤©æ¥å£  
- `get_available_providers()` - è·å–å¯ç”¨æä¾›å•†
- `set_default_provider(provider)` - è®¾ç½®é»˜è®¤æä¾›å•†

### æ•°æ®ç±»

#### LLMMessage
```python
@dataclass
class LLMMessage:
    role: str      # "system", "user", "assistant"
    content: str   # æ¶ˆæ¯å†…å®¹
```

#### LLMResponse  
```python
@dataclass
class LLMResponse:
    content: str                    # å›å¤å†…å®¹
    model: str                      # ä½¿ç”¨çš„æ¨¡å‹
    usage: Optional[Dict[str, Any]] # Tokenä½¿ç”¨æƒ…å†µ
    finish_reason: Optional[str]    # å®ŒæˆåŸå› 
```

## è¿è¡Œæµ‹è¯•

è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯é…ç½®ï¼š
```bash
python test_llm_router.py
```

æµ‹è¯•ä¼šéªŒè¯ï¼š
- âœ… æä¾›å•†è¿æ¥æ€§
- âœ… æ¨¡å‹åˆ‡æ¢
- âœ… æ¶ˆæ¯æ ¼å¼
- âœ… é”™è¯¯å¤„ç†

## åœ¨äº”å­æ£‹é¡¹ç›®ä¸­ä½¿ç”¨

åœ¨ `main.py` ä¸­ï¼ŒåŸæ¥çš„ `call_llm` å‡½æ•°å·²ç»è¢«æ›¿æ¢ä¸ºä½¿ç”¨LLMè·¯ç”±å™¨ï¼š

```python
def call_llm(system: str, user: str) -> str:
    """ä½¿ç”¨LLMè·¯ç”±å™¨è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹"""
    try:
        response = llm_router.simple_chat(system=system, user=user)
        return response
    except Exception as e:
        logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
        return ""
```

ç°åœ¨ä½ å¯ä»¥é€šè¿‡ä¿®æ”¹é…ç½®æ–‡ä»¶æ¥åˆ‡æ¢ä¸åŒçš„LLMæä¾›å•†ï¼Œè€Œæ— éœ€ä¿®æ”¹æ¸¸æˆä»£ç ã€‚

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ImportError: No module named 'config'**
   - ç¡®ä¿å·²ç»åˆ›å»ºäº† `config.py` æ–‡ä»¶
   - æˆ–è€…ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®

2. **APIè°ƒç”¨å¤±è´¥**
   - æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - æŸ¥çœ‹é”™è¯¯æ—¥å¿—äº†è§£è¯¦ç»†ä¿¡æ¯

3. **æ²¡æœ‰å¯ç”¨çš„æä¾›å•†**
   - ç¡®ä¿è‡³å°‘é…ç½®äº†ä¸€ä¸ªæœ‰æ•ˆçš„APIå¯†é’¥
   - æ£€æŸ¥é…ç½®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®

### è°ƒè¯•å»ºè®®

å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š
```python
import logging
logging.basicConfig(level=logging.INFO)
```

æŸ¥çœ‹å¯ç”¨æä¾›å•†ï¼š
```python
router = LLMRouter()
print("å¯ç”¨æä¾›å•†:", router.get_available_providers())
```

## æ‰©å±•å¼€å‘

è¦æ·»åŠ æ–°çš„LLMæä¾›å•†ï¼š

1. ç»§æ‰¿ `BaseLLMProvider` ç±»
2. å®ç° `chat_completion` æ–¹æ³•
3. åœ¨ `LLMRouter.PROVIDERS` ä¸­æ³¨å†Œ

```python
class NewProvider(BaseLLMProvider):
    def chat_completion(self, messages, model=None, **kwargs):
        # å®ç°APIè°ƒç”¨é€»è¾‘
        pass

# æ³¨å†Œæä¾›å•†
LLMRouter.PROVIDERS["newprovider"] = NewProvider
```